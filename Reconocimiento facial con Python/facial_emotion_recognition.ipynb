{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Emotion Recognition\n",
    "\n",
    "#### Facial Emotion Recognition using FER package\n",
    "\n",
    "\n",
    "Today, we will use our computer's camera to record videos and then, we will identify and analyze emotions using FER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Create program to use camera, record video and save it / You can skip this step and use a previous recorded video\n",
    "#### OpenCV requiered pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "location_videofile = 'C:/Users/jgali/Desktop/video_taller.avi'\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "codec = cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "\n",
    "writer = cv2.VideoWriter(location_videofile,codec,fps, (width, height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    writer.write(frame)\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyze video\n",
    "#Libraries required: FER y TENSORFLOW\n",
    "#Optional : pip install ipywidgets\n",
    "\n",
    "\n",
    "from fer import Video\n",
    "from fer import FER\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "face_detector = FER(mtcnn=True)\n",
    "input_video = Video(location_videofile)\n",
    "processing_data = input_video.analyze(face_detector, display = False)\n",
    "\n",
    "vid_df = input_video.to_pandas(processing_data)\n",
    "vid_df = input_video.get_first_face(vid_df)\n",
    "vid_df = input_video.get_emotions(vid_df)\n",
    "\n",
    "vid_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT\n",
    "\n",
    "fig = vid_df.plot(figsize = (20,10), fontsize = 10).get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESULTS\n",
    "\n",
    "angry = sum(vid_df.angry)\n",
    "disgust = sum(vid_df.disgust)\n",
    "fear = sum(vid_df.fear)\n",
    "happy = sum(vid_df.happy)\n",
    "sad = sum(vid_df.sad)\n",
    "surprise = sum(vid_df.surprise)\n",
    "neutral = sum(vid_df.neutral)\n",
    "\n",
    "emotions = ['angry','disgust','fear','happy','sad','surprise','neutral']\n",
    "values = [angry, disgust, fear,happy, sad,surprise,neutral]\n",
    "\n",
    "score = pd.DataFrame(emotions, columns = ['Emociones humanas'])\n",
    "score['puntaje'] = values\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2111ef7514d0cd31f42d5b8df48f0aadcba9168a8379e5e8e8c1e0a805943b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
